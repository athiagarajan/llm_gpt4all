{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDzPbp3WfaNP+yD0lr2waU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athiagarajan/llm_gpt4all/blob/main/GPT4AllBabySteps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmvG1x5E7UWv",
        "outputId": "a4e482f4-b4cb-4afa-9f6d-bd5965d008a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gpt4all in /usr/local/lib/python3.10/dist-packages (1.0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt4all) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt4all) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "pip install gpt4all\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gpt4all\n",
        "\n",
        "gptj = gpt4all.GPT4All(\"ggml-gpt4all-j-v1.3-groovy\")\n",
        "messages = [{\"role\": \"user\", \"content\": \"Give me a list of 10 colors and their RGB code\"}]\n",
        "ret = gptj.generate(\"The capital of France is \", max_tokens=3)\n",
        "print(ret)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04-MYdXh7aMP",
        "outputId": "4be7a6cb-24bb-41c0-8ca4-14fbdfdb7f8e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found model file at  /root/.cache/gpt4all/ggml-gpt4all-j-v1.3-groovy.bin\n",
            "\n",
            "Paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ret = gptj.generate(\"The capital of Canada is \", max_tokens=3)\n",
        "print(ret)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugBvdR_BYR8_",
        "outputId": "b362a03a-ee94-48f4-fcf1-b400585a9943"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Ottawa.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ret = gptj.generate(\"give me only the names of 5 rivers in India  \", max_tokens=100)\n",
        "print(ret)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02c4389d-9944-4ca5-e303-40dc5e363390",
        "id": "HtmaP3m0dpYH"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "- Yamuna River, Ganges River, Narmada River, Sabarimala River and Brahmaputra\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gpt4all\n",
        "gptj = gpt4all.GPT4All(\"orca-mini-3b.ggmlv3.q4_0.bin\")\n",
        "ret = gptj.generate(\"Give me a list of 10 colors and their RGB code\", max_tokens=12)\n",
        "print(ret)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52f1e26a-a373-4781-f8e7-753b829b67fa",
        "id": "D-Jr_2jZZFa8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found model file at  /root/.cache/gpt4all/orca-mini-3b.ggmlv3.q4_0.bin\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ret = gptj.generate(\"Give me a list of 10 colors and their RGB code\", max_tokens=15)\n",
        "print(ret)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePZc1VoEbZ0Z",
        "outputId": "2793e120-ebcf-4d0b-aa87-706db0470cce"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ", please.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = gptj.generate(\"The capital of France is \", max_tokens=3)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7iZEmrBcETW",
        "outputId": "04df9660-6a55-42f6-f730-a4fd0069ec4b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import gpt4all\n",
        "\n",
        "gptj = gpt4all.GPT4All(\"orca-mini-3b.ggmlv3.q4_0.bin\")\n",
        "messages = [{\"role\": \"user\", \"content\": \"Can you explain what is a large language model?\"}]\n",
        "ret = gptj.chat_completion(messages)\n",
        "\n",
        "messages.append(ret[\"choices\"][0][\"message\"])\n",
        "messages.append({\"role\": \"user\", \"content\": \"Can you give some examples applications?\"})\n",
        "ret = gptj.chat_completion(messages)\n",
        "\n",
        "messages.append(ret[\"choices\"][0][\"message\"])\n",
        "messages.append({\"role\": \"user\", \"content\": \"Are there any limitations?\"})\n",
        "ret = gptj.chat_completion(messages)\n",
        "\n",
        "messages.append(ret[\"choices\"][0][\"message\"])\n",
        "messages.append({\"role\": \"user\", \"content\": \"Summarize the above in two sentences.\"})\n",
        "ret = gptj.chat_completion(messages)\n",
        "\n",
        "print(json.dumps(messages, indent=4))\n",
        "print(json.dumps(ret, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "X982Em_k7aNz",
        "outputId": "0309f726-b250-4623-b9a6-46ab33d615ff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found model file at  /root/.cache/gpt4all/ggml-gpt4all-j-v1.3-groovy.bin\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-52130d4cdad2>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgpt4all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgptj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt4all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGPT4All\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ggml-gpt4all-j-v1.3-groovy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Can you explain what is a large language model?\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgptj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gpt4all/gpt4all.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name, model_path, model_type, allow_download, n_threads)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         )\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;31m# Set n_threads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_threads\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gpt4all/pyllmodel.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mllmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllmodel_loadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unable to instantiate model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}